- [Methodology](#methodology)
  * [Types of load](#types-of-load)
  * [Compared metrics](#compared-metrics)
    + [Trimmed mean vs median](#trimmed-mean-vs-median)
  * [Compared configurations](#compared-configurations)
- [Testbed](#testbed)
  * [Prometheus](#prometheus)
  * [Configurations](#configurations)
- [Benchmarks](#benchmarks)

<small><i><a href='http://ecotrust-canada.github.io/markdown-toc/'>Table of contents generated with markdown-toc</a></i></small>

## Methodology

TL;DR; you can jump right to [Benchmarks](#benchmarks) and look into methodology later.

### Types of load

There are three types of load to compare different aspects TCP proxies:

* `moderate load` - `25k RPS` (requests per second). Connections are being re-used for `50` requests.
  * In this mode we benchmark handling traffic over persisted connections.
  * Moderate request rate is chosen to benchmark proxies under _normal_ conditions. 
* `max load` - sending as many requests as the server can handle.
  * The intent is to test the proxies under stress conditions.
  * Also, we find the max throughput of the service (the saturation point).
* `no-keepalive` - using each connection for a single request 
  * So we can compare the performance characteristics of establishing new connections.
  * Establishing a connection is an expensive operation.
  * It involves resource allocation and dispatching tasks between worker threads.
  * As well as clean-up operations once a connection is closed. 
  
### Compared metrics

To compare different solutions, we use the following set of metrics:

* Latency (in microseconds, or `Âµs`)
  * `p50` (median) - a value that is greater than 50% of observed latency samples
  * `p90` - 90th percentile, or a value that is better than 9 out 10 latency samples. Usually a good proxy for a perceivable latency by humans.
  * `p99` - 99th percentile, the threshold for the worst 1% of samples.
  * tail-latencies: `p99.9` and `p99.99` - may be important for systems with multiple network hops or large fan-outs (e.g. a request gathers data from tens or hundreds microservices)
  * `max` - the worst-case. 
  * `tm99.9` - trimmed mean, or the mean value of all samples without the best and worst 0.1%. It is more useful than the traditional mean, as it removes a potentially disproportionate influence of outliers: https://en.wikipedia.org/wiki/Truncated_mean
  * `stddev` - the standard deviation of the latency. The lower the better: https://en.wikipedia.org/wiki/Standard_deviation
* Throughput `rps` (requests per second) 
* CPU utilization
* Memory utilization

In other words, we primarily focus on the latency, but also keep an eye on the cost of that latency in terms of CPU/Memory.
For the `max load` we also assess the maximum possible throughput of the system.

#### Trimmed mean vs median

Why do we need to observe trimmed mean if we already have median (i.e. `p50`)?
`p50` (or percentiles in general) may not necessarily capture performance regressions. For instance:

* `1,2,3,4,5,6,7,8,9,10` - `p50` is `5`, `trimmed mean` is `5`
* `5,5,5,5,5,6,7,8,9,10` - `p50` is still `5`, however the `trimmed mean` is `6.25`.

The same is applicable to any other percentile. If the team only uses `p90` or `p99` to monitor the performance of their system, they may miss dramatic regressions without being aware of that.

Of course, we may use multiple `fences` (`p10`, `p25`, etc.) - but why, if we can use a single metric?
In contrast, the traditional mean is susceptible to noise and outliers and not as good for capturing the general tendency. 

### Compared configurations

These benchmarks compare TCP proxies written in different languages, which use Non-blocking I/O.
Why TCP proxies? This is the simplest application dealing with the network I/O. All it does, is connection establishment and forwarding traffic.
Why Non-blocking I/O? You can read [this post](https://medium.com/swlh/distributed-systems-and-asynchronous-i-o-ef0f27655ce5) which tries to demonstrate why 
Non-blocking I/O is a much better option for network applications.

Let's say, you're building a network service. TCP proxy benchmarks are the lower boundary for the request latency it may have.
Everything else is added on top of that (e.g. parsing, validating, packing, traversing, construction of data, etc.).

So the following solutions are being compared:

* Baseline (`perf-gauge <-> nginx`) - direct communication to Nginx to establish the baseline: https://nginx.org/en/
* HAProxy (`perf-gauge <-> HAProxy <-> nginx`) - HAProxy in TCP-proxy mode. To compare to a mature solution written in `C`: http://www.haproxy.org/
* `draft-http-tunnel` - a simple C++ solution with very basic functionality (asio) (running in TCP mode): https://github.com/cmello/draft-http-tunnel/
* `http-tunnel` - a simple HTTPTunnel written in Rust (tokio) (running in TCP mode): https://github.com/xnuter/http-tunnel/
* `tcp-proxy` - a Golang solution: https://github.com/ickerwx/tcpproxy/
* `NetCrusher` - a Java solution (Java NIO): https://github.com/NetCrusherOrg/NetCrusher-java/
* `pproxy` - a Python solution based on `asyncio` (running in TCP Proxy mode): https://pypi.org/project/pproxy/

Thanks to [Cesar](https://github.com/cmello/) who coded the TCP proxy in C++ to make this benchmark possible.

## Testbed

Benchmarking network services is tricky, especially if we need to measure difference down to microseconds granularity.
To rule out network delays/noise we can try to employ one of the options:

* use co-located servers, e.g. VMs on the same physical machine, or in the same rack.
* use a single VM, but assign CPU cores to different components to avoid overlap 

Both are not ideal, but the latter seem to be an easier way. We just need to make sure, that the instance type is CPU optimized,
and it won't suffer from noisy-neighbor issues. In other words, it must have exclusive access to all cores as we're going to drive CPU utilization close to 100%.

E.g. if we use an 8-core machine, we can use the following assignment scheme:

* Cores 0-1: Nginx (serves `10kb` of payload per request)
* Cores 2-3: TCP proxy
* Cores 4-7: [perf-gauge](https://github.com/xnuter/perf-gauge) - the load-generator.

This can be achieved by using [cpu sets](https://codywu2010.wordpress.com/2015/09/27/cpuset-by-example/):

```
apt-get install cgroup-tools
```

Then we can create non-overlapping cpu-sets and run different components without competing for CPU and ruling out any network noise.

### Prometheus

`perf-gauge` can emit metrics to `Prometheus.` To launch a stack, you can use https://github.com/xnuter/prom-stack
I just forked `prom-stack` and removed anything but `prometheus,` `push-gateway` and `grafana.` You can clone the stack and launch `make.`

Then set the variable with the host, for instance:

```
export PROMETHEUS_HOST=10.138.0.2
```

### Configurations

Please note, that for all configuration we disable logging to minimize the number of variables, and the level of noise.

* [Perf-gauge](./perf-gauge-setup.md)
* [Nginx](nginx-config.md)
* TCP Proxies  
  * [HAProxy - C](haproxy-config.md)
  * [draft-http-tunnel - C++](cpp-config.md)
  * [http-tunnel - Rust](rust-config.md)
  * [tcp-roxy - Golang](golang-config.md)
  * [NetCrusher - Java](java-config.md)
  * [pproxy - Python](python-config.md)

## Benchmarks

Okay, we finally got to benchmark results. All benchmark results are split to two batches:

* Baseline, C, C++, Rust - comparing high-performance solutions
* Rust, Golang, Java, Python - comparing memory-safe languages

Yep, Rust belongs to both worlds. 

* [Moderate RPS](./moderate-tps.md)
* [Max RPS](./max-tps.md)
* [No keep-alive](./no-keepalive.md)